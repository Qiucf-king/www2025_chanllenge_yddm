{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier as cab\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def compress_csv_to_tar_gz(csv_files, output_filename):\n",
    "    # 创建 tar.gz 文件\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        for csv_file in csv_files:\n",
    "            # 确保文件存在\n",
    "            if os.path.isfile(csv_file):\n",
    "                tar.add(csv_file, arcname=os.path.basename(csv_file))\n",
    "            else:\n",
    "                print(f\"文件 {csv_file} 不存在，跳过。\")\n",
    "                \n",
    "def reduce_mem_usage(df, only_fp64=False, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    if only_fp64==True:\n",
    "        numerics = [ 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, train_y_2, sd):\n",
    "    folds = 5\n",
    "    seed = sd\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    # 初始化 GroupKFold，设置折数为 5\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    train = np.zeros((train_x.shape[0], len(np.unique(train_y))))  # 为多分类任务初始化\n",
    "    test = np.zeros((test_x.shape[0], len(np.unique(train_y))))    # 为多分类任务初始化\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "\n",
    "    model_lst = []\n",
    "    # 进行五折交叉验证\n",
    "#     for i, (train_index, valid_index) in enumerate(kf.split(train_x, groups=train_y_2)):\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i + 1)))\n",
    "        trn_x, trn_y = train_x.iloc[train_index], train_y[train_index]\n",
    "        val_x, val_y = train_x.iloc[valid_index], train_y[valid_index]\n",
    "#         sample_weight = y_train_sample_weight.iloc[train_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',  # multiclassova\n",
    "                'metric': 'multiclassova',  # 使用多分类的评价指标\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.1,\n",
    "                'seed': 2022,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 5000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=500, early_stopping_rounds=500,\n",
    "                            # feval=WeightedF1Metric,  # 使用自定义 F1 评分函数\n",
    "                             )\n",
    "            val_pred_proba = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred_proba = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "        elif clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x, label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',  # 修改为多分类\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'eval_metric': 'mlogloss',  # 使用多分类的评价指标\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.3\n",
    "                ,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2020,\n",
    "                'n_jobs': -1,\n",
    "                \"silent\": True,\n",
    "                'tree_method': 'gpu_hist',      # 使用 GPU 加速\n",
    "                'predictor': 'gpu_predictor',\n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, 8000, evals=watchlist, \n",
    "                              verbose_eval=500, early_stopping_rounds=500)\n",
    "            val_pred_proba = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred_proba = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "#             print(val_pred_proba[:10])\n",
    "#             print(val_pred_proba[:10, :10])\n",
    "\n",
    "        elif clf_name == \"cab\":\n",
    "            params = {\n",
    "                'learning_rate': 0.3,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 70,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'random_seed': 11251,\n",
    "                'depth': 5,\n",
    "                'task_type': 'GPU',  # 启用 GPU\n",
    "                'loss_function': 'MultiClassOneVsAll',  # 修改为多分类\n",
    "                \n",
    "            }\n",
    "            model = clf(iterations=8000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      use_best_model=True, verbose=500,\n",
    "                      cat_features = [],\n",
    "#                       sample_weight=sample_weight  # 添加样本权重\n",
    "#                       custom_metric=[f1_metric]  # 使用自定义多分类 F1 作为评估指标\n",
    "                     )\n",
    "            \n",
    "            # 获取概率预测\n",
    "            val_pred_proba = model.predict_proba(val_x)\n",
    "            test_pred_proba = model.predict_proba(test_x)\n",
    "\n",
    "        # 获取预测标签\n",
    "        val_pred = np.argmax(val_pred_proba, axis=1)\n",
    "#             test_labels = np.argmax(test_pred_proba, axis=1)\n",
    "\n",
    "        # 对于多分类，val_pred 和 test_pred 是类别的索引\n",
    "        train[valid_index] = val_pred_proba\n",
    "        test += test_pred_proba / kf.n_splits\n",
    "\n",
    "        # 计算 F1 分数（可以根据需要选择其他多分类指标）\n",
    "        f1 = f1_score(val_y, val_pred, average='micro')  # 使用加权平均 F1 分数\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "#         print(val_y.iloc[:10])\n",
    "#         print(val_pred_proba[:10,:10])\n",
    "\n",
    "        print(cv_scores)\n",
    "        model_lst.append(model)\n",
    "\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train, test, cv_scores, np.mean(cv_scores), np.std(cv_scores), model_lst\n",
    "\n",
    "def lgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(lgb, x_train, y_train, x_test, \"lgb\", train_y_2, sd)\n",
    "    return lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def cab_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(cab, x_train, y_train, x_test, \"cab\", train_y_2, sd)\n",
    "    return cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def xgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(xgb, x_train, y_train, x_test, \"xgb\", train_y_2, sd)\n",
    "    return xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============= V5特征已经完美复现V4最佳结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  5.45it/s]\n",
      " 67%|██████▋   | 4/6 [00:05<00:02,  1.31s/it]"
     ]
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v14'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df = pd.concat([train_df, df_], axis=0)\n",
    "    \n",
    "root_dir = '../data_fea_sub/v13'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "    \n",
    "test_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df = pd.concat([test_df, df_], axis=0)\n",
    "\n",
    "train_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df.columns.tolist()[3:]]\n",
    "test_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df.columns.tolist()[3:]]\n",
    "\n",
    "# train_df, test_df = train_df_v7, test_df_v7\n",
    "\n",
    "# train_df=train_df[train_df['t_cut'].isin([2,])].reset_index(drop=True)\n",
    "# test_df=test_df[test_df['t_cut'].isin([2,])].reset_index(drop=True)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = reduce_mem_usage(train_df)\n",
    "# test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5063112\n",
       "Name: t_cut, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['t_cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_df[train_df['filename'] =='train_X0.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('../data/train_y_v0.1.0.csv')\n",
    "cols_label = train_label.columns.tolist()[1:]\n",
    "\n",
    "# 将指定列拼接成新的一列\n",
    "train_label['label_combined'] = train_label[cols_label].apply(lambda row: ', '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_combined_unique = train_label['label_combined'].value_counts().index.tolist()\n",
    "dict_str2num= {k: v for k, v in zip(label_combined_unique, range(len(label_combined_unique)))}\n",
    "dict_num2str= {k: v for k, v in zip(range(len(label_combined_unique)), label_combined_unique)}\n",
    "\n",
    "len(label_combined_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label['label_combined_num'] = train_label['label_combined'].apply(lambda x: dict_str2num[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_label, on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515401, 138), (5063112, 42))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fea_count'] = train_df.groupby('filename')['filename'].transform('size')\n",
    "# test_df['fea_count'] = test_df.groupby('filename')['filename'].transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['fea_t_max','fea_a_mean', 'fea_dt_p2p'\n",
    "             , 'fea_dt_unicnt','fea_dt_skew','fea_dt_std', 'fea_dt_min', 'fea_dt_max'\n",
    "            ,'fea_dt_count'\n",
    "            ,'fea_t_min','fea_v_count','fea_a_count'\n",
    "            ,'fea_t_p2p','fea_dt_Q50'\n",
    "#              ,'fea_dt_rk'             \n",
    "            ]\n",
    "\n",
    "train_df = train_df.drop(cols_drop, axis=1)\n",
    "test_df = test_df.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 130.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->->->->->->->->->-> 去除唯一值前:  25 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q50', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt']\n",
      "->->->->->->->->->-> 唯一值特征:  0 []\n",
      "->->->->->->->->->-> 去除唯一值后:  25 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q50', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "col_label = 'label_combined_num'\n",
    "# 训练数据/测试数据准备\n",
    "features = [f for f in train_df.columns if 'fea_' in f and 'dv/dt' not in f\n",
    "            and '4rd' not in f and '5rd' not in f and 'fea_t_p2p_freq' not in f and 'fft_' not in f\n",
    "           and 'pos_' not in f and 'neg_' not in f\n",
    "           ]\n",
    "print('->'*10, '去除唯一值前: ', len(features), features)\n",
    "cols_single = []\n",
    "for col in tqdm(features):\n",
    "    unicnt = train_df[col].nunique()\n",
    "    if unicnt==1:\n",
    "        cols_single.append(col)\n",
    "print('->'*10, '唯一值特征: ', len(cols_single), cols_single)\n",
    "features = [f for f in features if f not in cols_single]\n",
    "print('->'*10, '去除唯一值后: ', len(features), features)\n",
    "\n",
    "# for col in features:\n",
    "#     num_5, num_95 = train_df[col].quantile(0.05), train_df[col].quantile(0.95)\n",
    "#     train_df[col] = train_df[col].apply(lambda x: num_5 if x<num_5 else num_95 if x>num_95 else x)\n",
    "\n",
    "            \n",
    "train = train_df.reset_index(drop=True).copy()\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x>-1 else -1)\n",
    "# train = train[train[col_label]!=0].reset_index(drop=True)\n",
    "test = test_df.reset_index(drop=True)\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "x_train = train[features]\n",
    "x_test = test[features]\n",
    "\n",
    "y_train = train[col_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15918, 157860)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['filename'].nunique(), test['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.to_csv('../data_tmp/x_train.csv', index=False)\n",
    "# x_test.to_csv('../data_tmp/x_test.csv', index=False)\n",
    "\n",
    "# y_train.to_csv('../data_tmp/y_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p',\n",
    "#        'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt',\n",
    "#        'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt',\n",
    "#        'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean',\n",
    "#        'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_v_Q50', 'fea_a_Q50']\n",
    "# len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_lst = []\n",
    "# cv_scores_lst = []\n",
    "# cv_scores_mean_lst = []\n",
    "# cv_scores_std_lst = []\n",
    "# cab_train_lst = []\n",
    "# cab_test_lst = []\n",
    "\n",
    "\n",
    "# col_bst = ''\n",
    "# cv_scores_bst = -1\n",
    "# cv_scores_mean_bst = -1\n",
    "# cv_scores_std_bst = 99\n",
    "# cab_train_bst = []\n",
    "# cab_test_bst = []\n",
    "\n",
    "# for col in tqdm(features):\n",
    "#     cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std = cab_model(x_train.drop(col, axis=1), y_train, x_test.drop(col, axis=1), y_train, sd)\n",
    "    \n",
    "#     col_lst.append(col)\n",
    "#     cv_scores_lst.append(cv_scores)\n",
    "#     cv_scores_mean_lst.append(cv_scores_mean)\n",
    "#     cv_scores_std_lst.append(cv_scores_std)\n",
    "#     cab_train_lst.append(cab_train)\n",
    "#     cab_test_lst.append(cab_test)\n",
    "    \n",
    "#     if cv_scores_mean>cv_scores_mean_bst and (cv_scores_std<=cv_scores_std_bst or (cv_scores_std>cv_scores_std_bst and cv_scores_std-cv_scores_std_bst<=0.001)):\n",
    "#         col_bst = col\n",
    "#         cv_scores_bst = cv_scores\n",
    "#         cv_scores_mean_bst = cv_scores_mean\n",
    "#         cv_scores_std_bst = cv_scores_std\n",
    "#         cab_train_bst = cab_train\n",
    "#         cab_test_bst = cab_test\n",
    "#         print('->'*20, 'fea_bst: {}, cv_scores_bst:{} , cv_scores_mean_bst:{}, cv_scores_std:{}'.format(col_bst,cv_scores_bst,cv_scores_mean_bst,cv_scores_std) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sd_lst = []\n",
    "# cv_scores_lst = []\n",
    "# cv_scores_mean_lst = []\n",
    "# cv_scores_std_lst = []\n",
    "# cab_train_lst = []\n",
    "# cab_test_lst = []\n",
    "\n",
    "\n",
    "# sd_bst = -1\n",
    "# cv_scores_bst = -1\n",
    "# cv_scores_mean_bst = -1\n",
    "# cv_scores_std_bst = 99\n",
    "# cab_train_bst = []\n",
    "# cab_test_bst = []\n",
    "\n",
    "# for sd in tqdm(range(100)):\n",
    "#     cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std = cab_model(x_train, y_train, x_test, y_train, sd)\n",
    "    \n",
    "#     sd_lst.append(sd)\n",
    "#     cv_scores_lst.append(cv_scores)\n",
    "#     cv_scores_mean_lst.append(cv_scores_mean)\n",
    "#     cv_scores_std_lst.append(cv_scores_std)\n",
    "#     cab_train_lst.append(cab_train)\n",
    "#     cab_test_lst.append(cab_test)\n",
    "    \n",
    "#     if cv_scores_mean>cv_scores_mean_bst and (cv_scores_std<=cv_scores_std_bst or (cv_scores_std>cv_scores_std_bst and cv_scores_std-cv_scores_std_bst<=0.001)):\n",
    "#         sd_bst = sd\n",
    "#         cv_scores_bst = cv_scores\n",
    "#         cv_scores_mean_bst = cv_scores_mean\n",
    "#         cv_scores_std_bst = cv_scores_std\n",
    "#         cab_train_bst = cab_train\n",
    "#         cab_test_bst = cab_test\n",
    "#         print('->'*20, 'sd_bst: {}, cv_scores_bst:{} , cv_scores_mean_bst:{}, cv_scores_std:{}'.format(sd_bst,cv_scores_bst,cv_scores_mean_bst,cv_scores_std) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "\n",
    "# # 创建 StandardScaler 对象\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # 使用 X_train 拟合 scaler\n",
    "# scaler.fit(x_train)\n",
    "\n",
    "# # 保存 scaler 对象\n",
    "# joblib.dump(scaler, '../model/scaler_v7.pkl')\n",
    "\n",
    "# # 使用拟合好的 scaler 转换\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "# x_train = pd.DataFrame(x_train, columns = features)\n",
    "# x_test = pd.DataFrame(x_test, columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p',\n",
       "       'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew',\n",
       "       'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p',\n",
       "       'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q50', 'fea_a_Q75', 'fea_a_skew',\n",
       "       'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "0:\tlearn: 0.4464535\ttest: 0.4464598\tbest: 0.4464598 (0)\ttotal: 41.2ms\tremaining: 5m 29s\n",
      "500:\tlearn: 0.0129499\ttest: 0.0132466\tbest: 0.0132466 (500)\ttotal: 16.7s\tremaining: 4m 9s\n",
      "1000:\tlearn: 0.0121183\ttest: 0.0126366\tbest: 0.0126366 (1000)\ttotal: 33.6s\tremaining: 3m 55s\n",
      "1500:\tlearn: 0.0116794\ttest: 0.0124094\tbest: 0.0124094 (1500)\ttotal: 50.9s\tremaining: 3m 40s\n",
      "2000:\tlearn: 0.0113957\ttest: 0.0123148\tbest: 0.0123148 (2000)\ttotal: 1m 8s\tremaining: 3m 24s\n",
      "2500:\tlearn: 0.0111597\ttest: 0.0122484\tbest: 0.0122484 (2500)\ttotal: 1m 26s\tremaining: 3m 9s\n",
      "3000:\tlearn: 0.0109626\ttest: 0.0122095\tbest: 0.0122093 (2998)\ttotal: 1m 43s\tremaining: 2m 52s\n",
      "3500:\tlearn: 0.0107990\ttest: 0.0121885\tbest: 0.0121884 (3494)\ttotal: 2m 1s\tremaining: 2m 35s\n",
      "bestTest = 0.01217305487\n",
      "bestIteration = 3839\n",
      "Shrink model to first 3840 iterations.\n"
     ]
    }
   ],
   "source": [
    "sd = 518885135\n",
    "\n",
    "cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, cab_model_lst = cab_model(x_train, y_train, x_test, train_df['filename'].values, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[0]\ttrain-mlogloss:1.72947\teval-mlogloss:1.73126\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 500 rounds.\n",
      "[500]\ttrain-mlogloss:0.56726\teval-mlogloss:0.62343\n",
      "[1000]\ttrain-mlogloss:0.52926\teval-mlogloss:0.59997\n",
      "[1500]\ttrain-mlogloss:0.51743\teval-mlogloss:0.59632\n",
      "[2000]\ttrain-mlogloss:0.51016\teval-mlogloss:0.59459\n",
      "[2500]\ttrain-mlogloss:0.50429\teval-mlogloss:0.59340\n",
      "[3000]\ttrain-mlogloss:0.49962\teval-mlogloss:0.59238\n",
      "[3500]\ttrain-mlogloss:0.49593\teval-mlogloss:0.59187\n",
      "[4000]\ttrain-mlogloss:0.49271\teval-mlogloss:0.59163\n",
      "[4500]\ttrain-mlogloss:0.48989\teval-mlogloss:0.59139\n",
      "[5000]\ttrain-mlogloss:0.48752\teval-mlogloss:0.59110\n",
      "[5500]\ttrain-mlogloss:0.48527\teval-mlogloss:0.59093\n",
      "[6000]\ttrain-mlogloss:0.48335\teval-mlogloss:0.59088\n"
     ]
    }
   ],
   "source": [
    "xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, xgb_model_lst = xgb_model(x_train, y_train, x_test, y_train, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.45s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(cab_model_lst))):\n",
    "    joblib.dump(cab_model_lst[i], '../model/v51_sd518885135_feacut_10drop_merge_v13_cab_{}.pkl'.format(i))\n",
    "\n",
    "# test = np.zeros((x_test.shape[0], len(np.unique(y_train))))    # 为多分类任务初始化\n",
    "# for i in tqdm(range(5)):\n",
    "#     model = joblib.load( '../model/v51_sd518885135_feacut_10drop_merge_v41_cab_{}.pkl'.format(i))\n",
    "#     print('load model {}'.format(i))\n",
    "#     test_pred_proba = model.predict_proba(x_test)\n",
    "#     test += test_pred_proba / 5\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(xgb_model_lst))):\n",
    "    joblib.dump(xgb_model_lst[i], '../model/v51_sd518885135_feacut_10drop_merge_v13_xgb_{}.pkl'.format(i))\n",
    "    \n",
    "# test = np.zeros((x_test.shape[0], len(np.unique(y_train))))    # 为多分类任务初始化\n",
    "# for i in tqdm(range(5)):\n",
    "#     model = joblib.load('../model/v51_sd518885135_feacut_10drop_merge_v43_xgb_{}.pkl'.format(i) )\n",
    "#     print('load model {}'.format(i))\n",
    "#     test_pred_proba = model.predict(xgb.DMatrix(x_test), ntree_limit=model.best_ntree_limit)\n",
    "#     test += test_pred_proba / 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 合并概率矩阵和 file 列\n",
    "combined_df = pd.concat([test_df[['filename']].reset_index(drop=True), pd.DataFrame(1.0*cab_test )], axis=1)\n",
    "# combined_df = pd.concat([test_df[['filename']].reset_index(drop=True), pd.DataFrame(0.5*cab_test +0.5*xgb_test )], axis=1)\n",
    "\n",
    "# 根据 'file' 列分组并计算 n 列的最大值\n",
    "max_values = combined_df.groupby('filename').mean().reset_index()\n",
    "print(max_values.shape)\n",
    "test_df_out = max_values.copy()\n",
    "print(max_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_label = 'label_combined_num'\n",
    "test_df_out[col_label] = np.argmax(max_values.iloc[:, 1:].values, axis=1)\n",
    "test_df_out['prob'] = np.max(max_values.iloc[:, 1:].values, axis=1)\n",
    "print(test_df_out[col_label].value_counts().head())\n",
    "\n",
    "file = 'v51_sd518885135_feacut_10drop_merge_v13_cab_5fold_mean'\n",
    "test_df_out['label_combined'] = test_df_out['label_combined_num'].apply(lambda x: dict_num2str[x])\n",
    "for i in tqdm(range(len(cols_label))):\n",
    "    test_df_out[cols_label[i]] = test_df_out['label_combined'].apply(lambda x: int(x.split(',')[i])/2+0.5 )\n",
    "print(test_df_out['Zone_Air_Temperature_Sensor'].value_counts())\n",
    "\n",
    "for col in tqdm(cols_label):\n",
    "    test_df_out[col] = test_df_out[col].apply(lambda x: round(x,4))\n",
    "cols_out = ['filename'] + cols_label\n",
    "test_df_out[cols_out].to_csv('../res/{}.csv'.format(file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例 CSV 文件列表\n",
    "csv_files = ['../res/{}.csv'.format(file)]  # 替换为你的 CSV 文件名\n",
    "output_filename = '../tar/{}.tar.gz'.format(file)\n",
    "\n",
    "# 调用函数\n",
    "compress_csv_to_tar_gz(csv_files, output_filename)\n",
    "\n",
    "print(f\"已将 CSV 文件压缩为 {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0    70397\n",
    "1    37300\n",
    "2    34790\n",
    "9    13737\n",
    "5    13590\n",
    "Name: label_combined_num, dtype: int64\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [01:39<00:00,  1.06s/it]\n",
    "0.0    304271\n",
    "1.0      9839\n",
    "0.5      1610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df_bef = pd.read_csv('../res/v41_sd61_feacut_10drop_merge_v14_cab_5fold.csv')\n",
    "df_aft = pd.read_csv('../res/{}.csv'.format(file) )\n",
    "\n",
    "df_bef.shape, df_aft.shape\n",
    "df_mge = pd.concat([df_bef, df_aft], axis=0)\n",
    "df_mge = df_mge.groupby('filename').tail(1)\n",
    "\n",
    "file_fix = '{}_fix'.format(file)\n",
    "df_mge.to_csv('../res/{}.csv'.format(file_fix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ../res/v51_sd518885135_feacut_10drop_merge_v43_5cab5xgb_5fold_max_fix.csv to ../tar/v51_sd518885135_feacut_10drop_merge_v43_5cab5xgb_5fold_max_fix.tar.gz\n",
      "File ../res/v51_sd518885135_feacut_10drop_merge_v43_5cab5xgb_5fold_max_fix.csv has been compressed into ../tar/v51_sd518885135_feacut_10drop_merge_v43_5cab5xgb_5fold_max_fix.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 指定要压缩的CSV文件路径\n",
    "csv_file = '../res/{}.csv'.format(file_fix)\n",
    "\n",
    "# 指定输出的tar.gz文件路径\n",
    "output_tar_gz_path = '../tar/{}.tar.gz'.format(file_fix)\n",
    "\n",
    "# 检查CSV文件是否存在\n",
    "if not os.path.exists(csv_file):\n",
    "    print(f'File {csv_file} does not exist.')\n",
    "else:\n",
    "    # 创建一个tar.gz文件\n",
    "    with tarfile.open(output_tar_gz_path, 'w:gz') as tar:\n",
    "        # 添加文件到tar归档中\n",
    "        tar.add(csv_file, arcname=os.path.basename(csv_file))\n",
    "        print(f'Added {csv_file} to {output_tar_gz_path}')\n",
    "\n",
    "    print(f'File {csv_file} has been compressed into {output_tar_gz_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_prob = {\n",
    "#             'cab_train':cab_train, \n",
    "            'cab_test':cab_test,\n",
    "#             'xgb_train':xgb_train, \n",
    "            'xgb_test':xgb_test,\n",
    "            'dict_num2str':dict_num2str, 'dict_str2num':dict_str2num, 'cols_label':cols_label}\n",
    "\n",
    "\n",
    "# 保存为 .npy 文件\n",
    "np.save('../res/v51_sd518885135_feacut_10drop_merge_v12_5fold.npy', dict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v41_cab 0.441\t0.417\t0.576\t0.411\t0.987\t\n",
    "# v41_lgb 0.399\t0.343\t0.445\t0.397\t0.988\n",
    "# v41_xgb 0.444\t0.399\t0.541\t0.414\t0.987\n",
    "\n",
    "# v41_8cab_2lgb      0.446\t0.427\t0.594\t0.416\t0.987\n",
    "# v41_7cab_3lgb      0.449\t0.428\t0.598\t0.419\t0.987\n",
    "# v41_7xgb_3cab      0.457\t0.430\t0.595\t0.423\t0.987\n",
    "# v41_6xgb_4cab      0.461\t0.434\t0.600\t0.426\t0.987\n",
    "# v41_5xgb_5cab      0.461\t0.434\t0.600\t0.426\t0.987\n",
    "# v41_4xgb_4cab_2lgb 0.460\t0.428\t0.590\t0.427\t0.987\n",
    "# v41_sd61_feacut_1_5cab_5xgb 0.469\t0.439\t0.603\t0.432\t0.987\n",
    "# v41_sd61_feacut_1_6cab_4xgb 0.470\t0.435\t0.598\t0.432\t0.987\n",
    "\n",
    "# v41_sd61_feacut_6drop_6xgb_4cab 0.470\t0.433\t0.591\t0.434\t0.988\n",
    "\n",
    "\n",
    "# v42_cab 0.429\t0.414\t0.588\t0.397\t0.987\n",
    "# v41-sd1024_cab 0.441\t0.423\t0.596\t0.411\t0.987\t\n",
    "# v41-sd61_cab 0.444\t0.423\t0.590\t0.413\t0.987\n",
    "\n",
    "# v41_sd61_feacut_1_lgb 0.398\t0.340\t0.441\t0.397\t0.988\n",
    "# v41_sd61_feacut_3_cab 0.448\t0.416\t0.575\t0.421\t0.987\n",
    "# v41-sd61_feacut_6drop_cab 0.454\t0.422\t0.583\t0.425\t0.987\n",
    "# v41-sd61_feacut_7drop_cab 0.455\t0.424\t0.584\t0.428\t0.987\n",
    "\n",
    "# v41-sd61_feacut_7drop_v4_cab 0.468\t0.433\t0.595\t0.437\t0.987\n",
    "# v41_sd61_feacut_7drop_v4_5cab_5xgb\t0.479\t0.445\t0.606\t0.438\t0.988\n",
    "\n",
    "# v41_sd61_feacut_10drop_v1_cab 0.471\t0.436\t0.595\t0.439\t0.987\n",
    "# v41_sd61_feacut_10drop_v1_5cab_5xgb 0.482\t0.450\t0.617\t0.442\t0.987\n",
    "# v41_sd61_feacut_10drop_v1_55cab_45xgb\t0.483\t0.451\t0.619\t0.443\t0.987\t\n",
    "# v41_sd61_feacut_10drop_v2_cab_5fold \t0.487\t0.446\t0.601\t0.457\t0.987\n",
    "# v41_sd61_feacut_10drop_v23_cab_5fold 0.487\t0.443\t0.588\t0.468\t0.988\n",
    "# v41_sd61_feacut_10drop_v25_cab_5fold 0.489\t0.448\t0.593\t0.469\t0.988\n",
    "# v41_sd61_feacut_10drop_merge_v1_55cab_45xgb 0.498\t0.461\t0.617\t0.465\t0.988\n",
    "# v41_sd61_feacut_10drop_merge_v12_cab_5fold 0.495\t0.453\t0.606\t0.471\t0.988\n",
    "\n",
    "# v41_sd61_feacut_10drop_merge_v13_cab_5fold 0.496\t0.456\t0.604\t0.472\t0.988\n",
    "\n",
    "\n",
    "# v41_sd61_feacut_10drop_merge_v24_cab_5fold_fix 0.567\t0.516\t0.644\t0.557\t0.989\n",
    "# v41_sd61_feacut_10drop_merge_v25_cab_5fold_fix 0.570\t0.514\t0.642\t0.563\t0.989\t\n",
    "# v41_sd61_feacut_10drop_merge_v26_cab_5fold_fix 0.575\t0.518\t0.647\t0.568\t0.989\n",
    "\n",
    "# v41_sd61_feacut_10drop_merge_v27_cab_5fold_fix 0.576\t0.520\t0.649\t0.570\t0.989\t\n",
    "# v41_sd61_feacut_10drop_merge_v28_cab_5fold_fix 0.579\t0.522\t0.648\t0.572\t0.989\n",
    "\n",
    "# v51_sd518885_feacut_10drop_merge_v1_cab_5fold         0.580\t0.523\t0.648\t0.574\t0.989\t\n",
    "# v51_sd518885_feacut_10drop_merge_v1_cab_5fold_mean\t0.580\t0.531\t0.652\t0.570\t0.989\t\n",
    "# v51_sd518885_feacut_10drop_merge_v22_cab_5fold_max  0.580\t0.524\t0.650\t0.576\t0.989\n",
    "# v51_sd518885_feacut_10drop_merge_v22_cab_5fold_mean 0.581\t0.533\t0.656\t0.572\t0.990\t\n",
    "\n",
    "# v51_sd518885135_feacut_10drop_merge_v22_cab_5fold_mean 0.582\t0.535\t0.657\t0.573\t0.990\n",
    "\n",
    "# v51_sd518885135_feacut_10drop_merge_v22_5cab5xgb_5fold_mean 0.591 0.552 0.683 0.571 0.990\n",
    "\n",
    "\n",
    "# v51_sd518885135_feacut_10drop_merge_v41_cab_5fold_mean 0.586\t0.534\t0.659\t0.579\t0.990\t\n",
    "# v51_sd518885135_feacut_10drop_merge_v41_xgb_5fold_mean 0.582\t0.537\t0.671\t0.564\t0.990\n",
    "# v51_sd518885135_feacut_10drop_merge_v41_6cab4xgb_5fold_mean 0.594 0.549 0.677 0.578 0.990\n",
    "\n",
    "# v51_sd518885135_feacut_10drop_merge_v42_xgb_5fold_mean 0.583\t0.540\t0.671\t0.564\t0.989\n",
    "# v51_sd518885135_feacut_10drop_merge_v42_6cab4xgb_5fold_mean 0.594\t0.551\t0.680\t0.577\t0.990\t\n",
    "# v51_sd518885135_feacut_10drop_merge_v42_5cab5xgb_5fold_max 0.598\t0.545\t0.673\t0.585\t0.989\n",
    "# v51_sd518885135_feacut_10drop_merge_v42_55cab45xgb_5fold_max 0.598\t0.544\t0.672\t0.587\t0.989\n",
    "\n",
    "# v51_sd518885135_feacut_10drop_merge_v43_5cab5xgb_5fold_max\t0.598\t0.545\t0.673\t0.586\t0.989\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全量数据 切片\n",
    "# v51_sd518885135_feacut_10drop_merge_v10_cab_5fold_max\t0.554\t0.504\t0.642\t0.544\t0.989\n",
    "# v51_sd518885135_feacut_10drop_merge_v10_5cab5xgb_5fold_max 0.557\t0.512\t0.655\t0.539\t0.989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_prob = np.load('../res/v41_lgb_cab_xgb.npy', allow_pickle=True).item()\n",
    "\n",
    "# lgb_train = dict_prob['lgb_train']\n",
    "# cab_train = dict_prob['cab_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost                            1.0.1\n",
    "# lightgbm                           3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
