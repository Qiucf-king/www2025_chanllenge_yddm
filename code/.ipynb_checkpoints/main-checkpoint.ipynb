{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier as cab\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def compress_csv_to_tar_gz(csv_files, output_filename):\n",
    "    # 创建 tar.gz 文件\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        for csv_file in csv_files:\n",
    "            # 确保文件存在\n",
    "            if os.path.isfile(csv_file):\n",
    "                tar.add(csv_file, arcname=os.path.basename(csv_file))\n",
    "            else:\n",
    "                print(f\"文件 {csv_file} 不存在，跳过。\")\n",
    "                \n",
    "def reduce_mem_usage(df, only_fp64=False, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    if only_fp64==True:\n",
    "        numerics = [ 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, train_y_2, sd):\n",
    "    folds = 5\n",
    "    seed = sd\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    # 初始化 GroupKFold，设置折数为 5\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "    nclass =  len(np.unique(train_y))\n",
    "    train = np.zeros((train_x.shape[0],nclass ))  # 为多分类任务初始化\n",
    "    test = np.zeros((test_x.shape[0], nclass ))    # 为多分类任务初始化\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "\n",
    "    model_lst = []\n",
    "    # 进行五折交叉验证\n",
    "#     for i, (train_index, valid_index) in enumerate(kf.split(train_x, groups=train_y_2)):\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i + 1)))\n",
    "        trn_x, trn_y = train_x.iloc[train_index], train_y[train_index]\n",
    "        val_x, val_y = train_x.iloc[valid_index], train_y[valid_index]\n",
    "#         sample_weight = y_train_sample_weight.iloc[train_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',  # multiclassova\n",
    "                'metric': 'multiclassova',  # 使用多分类的评价指标\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.1,\n",
    "                'seed': 2022,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 5000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=500, early_stopping_rounds=500,\n",
    "                            # feval=WeightedF1Metric,  # 使用自定义 F1 评分函数\n",
    "                             )\n",
    "            val_pred_proba = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred_proba = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "        elif clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x, label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',  # 修改为多分类\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'eval_metric': 'mlogloss',  # 使用多分类的评价指标\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.3\n",
    "                ,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2020,\n",
    "                'n_jobs': -1,\n",
    "                \"silent\": True,\n",
    "                'tree_method': 'gpu_hist',      # 使用 GPU 加速\n",
    "                'predictor': 'gpu_predictor',\n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, 9000, evals=watchlist, \n",
    "                              verbose_eval=500, early_stopping_rounds=500)\n",
    "            val_pred_proba = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred_proba = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "#             print(val_pred_proba[:10])\n",
    "#             print(val_pred_proba[:10, :10])\n",
    "\n",
    "        elif clf_name == \"cab\":\n",
    "            params = {\n",
    "                'learning_rate': 0.3,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 70,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'random_seed': 11251,\n",
    "                'depth': 5,\n",
    "                'task_type': 'GPU',  # 启用 GPU\n",
    "                'loss_function': 'MultiClassOneVsAll',  # 修改为多分类\n",
    "                \n",
    "            }\n",
    "            model = clf(iterations=9000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      use_best_model=True, verbose=500,\n",
    "                      cat_features = [],\n",
    "#                       sample_weight=sample_weight  # 添加样本权重\n",
    "#                       custom_metric=[f1_metric]  # 使用自定义多分类 F1 作为评估指标\n",
    "                     )\n",
    "            \n",
    "            # 获取概率预测\n",
    "            val_pred_proba = model.predict_proba(val_x)\n",
    "            # test_pred_proba = model.predict_proba(test_x)\n",
    "            \n",
    "            \n",
    "            # 定义批大小\n",
    "            batch_size = 2000000  # 根据您的内存情况进行调整\n",
    "\n",
    "            # 计算总的样本数\n",
    "            num_samples = test_x.shape[0]\n",
    "\n",
    "            # 初始化一个空的数组来存储预测结果\n",
    "            test_pred_proba = np.empty((num_samples, nclass ))  # 假设 model.classes_ 返回类别数\n",
    "\n",
    "            # 分批次进行预测\n",
    "            for start in tqdm(range(0, num_samples, batch_size) ):\n",
    "                end = min(start + batch_size, num_samples)  # 确保不超出边界\n",
    "                print(start, end)\n",
    "\n",
    "                # 进行预测并直接存储到预测结果数组中\n",
    "                test_pred_proba[start:end] = model.predict_proba(test_x[start:end] )\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "        # 获取预测标签\n",
    "        val_pred = np.argmax(val_pred_proba, axis=1)\n",
    "#             test_labels = np.argmax(test_pred_proba, axis=1)\n",
    "\n",
    "        # 对于多分类，val_pred 和 test_pred 是类别的索引\n",
    "        train[valid_index] = val_pred_proba\n",
    "        test += test_pred_proba / kf.n_splits\n",
    "        \n",
    "\n",
    "\n",
    "        # 计算 F1 分数（可以根据需要选择其他多分类指标）\n",
    "        f1 = f1_score(val_y, val_pred, average='micro')  # 使用加权平均 F1 分数\n",
    "        cv_scores.append(f1)\n",
    "\n",
    "#         print(val_y.iloc[:10])\n",
    "#         print(val_pred_proba[:10,:10])\n",
    "\n",
    "        print(cv_scores)\n",
    "        model_lst.append(model)\n",
    "        \n",
    "        del test_pred_proba, val_pred_proba, val_pred\n",
    "        [gc.collect() for _ in range(5)]\n",
    "\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train, test, cv_scores, np.mean(cv_scores), np.std(cv_scores), model_lst\n",
    "\n",
    "def lgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(lgb, x_train, y_train, x_test, \"lgb\", train_y_2, sd)\n",
    "    return lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def cab_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(cab, x_train, y_train, x_test, \"cab\", train_y_2, sd)\n",
    "    return cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def xgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(xgb, x_train, y_train, x_test, \"xgb\", train_y_2, sd)\n",
    "    return xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1038169, 42), (10135596, 42))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "#     print('{}/{}'.format(root_dir, file))\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df = pd.concat([train_df, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df = pd.concat([test_df, df_], axis=0)\n",
    "\n",
    "train_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df.columns.tolist()[3:]]\n",
    "test_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df.columns.tolist()[3:]]\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038087, 41) (10121312, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_2'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_2'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038060, 51) (10116842, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_3'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_3'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038033, 39) (10112887, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_4'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_4'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df_2 = train_df_2.iloc[:, :40]\n",
    "test_df_2 = test_df_2.iloc[:, :40]\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = reduce_mem_usage(train_df)\n",
    "# test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['t_cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('../data/train_y_v0.1.0.csv')\n",
    "cols_label = train_label.columns.tolist()[1:]\n",
    "\n",
    "# 将指定列拼接成新的一列\n",
    "train_label['label_combined'] = train_label[cols_label].apply(lambda row: ', '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_combined_unique = train_label['label_combined'].value_counts().index.tolist()\n",
    "dict_str2num= {k: v for k, v in zip(label_combined_unique, range(len(label_combined_unique)))}\n",
    "dict_num2str= {k: v for k, v in zip(range(len(label_combined_unique)), label_combined_unique)}\n",
    "\n",
    "len(label_combined_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label['label_combined_num'] = train_label['label_combined'].apply(lambda x: dict_str2num[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_label, on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1038169, 260), (10135596, 164))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fea_count'] = train_df.groupby('filename')['filename'].transform('size')\n",
    "# test_df['fea_count'] = test_df.groupby('filename')['filename'].transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['fea_t_max','fea_a_mean', 'fea_dt_p2p'\n",
    "             , 'fea_dt_unicnt','fea_dt_skew','fea_dt_std', 'fea_dt_min', 'fea_dt_max'\n",
    "            ,'fea_dt_count'\n",
    "            ,'fea_t_min','fea_v_count','fea_a_count'\n",
    "            ,'fea_t_p2p','fea_dt_Q50'\n",
    "             \n",
    "             \n",
    "             , 'fea_a_2_unicnt', 'fea_a_3_unicnt', 'fea_a_4_unicnt'\n",
    "             , 'fea_a_2_max', 'fea_a_3_max', 'fea_a_4_max'\n",
    "             , 'fea_a_Q50', 'fea_a_Q50', 'fea_a_Q50'\n",
    "#              ,'fea_dt_rk'             \n",
    "            ]\n",
    "\n",
    "train_df = train_df.drop(cols_drop, axis=1)\n",
    "test_df = test_df.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->->->->->->->->->-> 去除唯一值前:  116 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_a_2_sum', 'fea_a_2_mean', 'fea_a_2_std', 'fea_a_2_min', 'fea_a_2_p2p', 'fea_a_2_Q25', 'fea_a_2_Q50', 'fea_a_2_Q75', 'fea_a_2_skew', 'fea_a_2_kurt', 'fea_a_3_sum', 'fea_a_3_mean', 'fea_a_3_std', 'fea_a_3_min', 'fea_a_3_p2p', 'fea_a_3_Q25', 'fea_a_3_Q50', 'fea_a_3_Q75', 'fea_a_3_skew', 'fea_a_3_kurt', 'fea_a_4_sum', 'fea_a_4_mean', 'fea_a_4_std', 'fea_a_4_min', 'fea_a_4_p2p', 'fea_a_4_Q25', 'fea_a_4_Q50', 'fea_a_4_Q75', 'fea_a_4_skew', 'fea_a_4_kurt', 'fea_v_sum', 'fea_a_sum', 'fea_a_5_sum', 'fea_a_5_mean', 'fea_a_5_std', 'fea_a_5_max', 'fea_a_5_min', 'fea_a_5_p2p', 'fea_a_5_unicnt', 'fea_a_5_Q25', 'fea_a_5_Q50', 'fea_a_5_Q75', 'fea_a_5_skew', 'fea_a_5_kurt', 'fea_da_1_sum', 'fea_da_1_mean', 'fea_da_1_std', 'fea_da_1_max', 'fea_da_1_min', 'fea_da_1_p2p', 'fea_da_1_unicnt', 'fea_da_1_Q25', 'fea_da_1_Q50', 'fea_da_1_Q75', 'fea_da_1_skew', 'fea_da_1_kurt', 'fea_da_2_sum', 'fea_da_2_mean', 'fea_da_2_std', 'fea_da_2_max', 'fea_da_2_min', 'fea_da_2_p2p', 'fea_da_2_unicnt', 'fea_da_2_Q25', 'fea_da_2_Q50', 'fea_da_2_Q75', 'fea_da_2_skew', 'fea_da_2_kurt', 'fea_da_3_sum', 'fea_da_3_mean', 'fea_da_3_std', 'fea_da_3_max', 'fea_da_3_min', 'fea_da_3_p2p', 'fea_da_3_unicnt', 'fea_da_3_Q25', 'fea_da_3_Q50', 'fea_da_3_Q75', 'fea_da_3_skew', 'fea_da_3_kurt', 'fea_a_6_sum', 'fea_a_6_mean', 'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p', 'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75', 'fea_a_6_skew', 'fea_a_6_kurt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [00:03<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->->->->->->->->->-> 唯一值特征:  0 []\n",
      "->->->->->->->->->-> 去除唯一值后:  116 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_a_2_sum', 'fea_a_2_mean', 'fea_a_2_std', 'fea_a_2_min', 'fea_a_2_p2p', 'fea_a_2_Q25', 'fea_a_2_Q50', 'fea_a_2_Q75', 'fea_a_2_skew', 'fea_a_2_kurt', 'fea_a_3_sum', 'fea_a_3_mean', 'fea_a_3_std', 'fea_a_3_min', 'fea_a_3_p2p', 'fea_a_3_Q25', 'fea_a_3_Q50', 'fea_a_3_Q75', 'fea_a_3_skew', 'fea_a_3_kurt', 'fea_a_4_sum', 'fea_a_4_mean', 'fea_a_4_std', 'fea_a_4_min', 'fea_a_4_p2p', 'fea_a_4_Q25', 'fea_a_4_Q50', 'fea_a_4_Q75', 'fea_a_4_skew', 'fea_a_4_kurt', 'fea_v_sum', 'fea_a_sum', 'fea_a_5_sum', 'fea_a_5_mean', 'fea_a_5_std', 'fea_a_5_max', 'fea_a_5_min', 'fea_a_5_p2p', 'fea_a_5_unicnt', 'fea_a_5_Q25', 'fea_a_5_Q50', 'fea_a_5_Q75', 'fea_a_5_skew', 'fea_a_5_kurt', 'fea_da_1_sum', 'fea_da_1_mean', 'fea_da_1_std', 'fea_da_1_max', 'fea_da_1_min', 'fea_da_1_p2p', 'fea_da_1_unicnt', 'fea_da_1_Q25', 'fea_da_1_Q50', 'fea_da_1_Q75', 'fea_da_1_skew', 'fea_da_1_kurt', 'fea_da_2_sum', 'fea_da_2_mean', 'fea_da_2_std', 'fea_da_2_max', 'fea_da_2_min', 'fea_da_2_p2p', 'fea_da_2_unicnt', 'fea_da_2_Q25', 'fea_da_2_Q50', 'fea_da_2_Q75', 'fea_da_2_skew', 'fea_da_2_kurt', 'fea_da_3_sum', 'fea_da_3_mean', 'fea_da_3_std', 'fea_da_3_max', 'fea_da_3_min', 'fea_da_3_p2p', 'fea_da_3_unicnt', 'fea_da_3_Q25', 'fea_da_3_Q50', 'fea_da_3_Q75', 'fea_da_3_skew', 'fea_da_3_kurt', 'fea_a_6_sum', 'fea_a_6_mean', 'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p', 'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75', 'fea_a_6_skew', 'fea_a_6_kurt']\n"
     ]
    }
   ],
   "source": [
    "col_label = 'label_combined_num'\n",
    "# 训练数据/测试数据准备\n",
    "features = [f for f in train_df.columns if 'fea_' in f and 'dv/dt' not in f\n",
    "            and '4rd' not in f and '5rd' not in f and 'fea_t_p2p_freq' not in f and 'fft_' not in f\n",
    "           and 'pos_' not in f and 'neg_' not in f\n",
    "            \n",
    "            and 'a_7' not in f\n",
    "            \n",
    "            and 'da_4' not in f \n",
    "            and 'da_5' not in f \n",
    "            and 'd2a' not in f\n",
    "           ]\n",
    "print('->'*10, '去除唯一值前: ', len(features), features)\n",
    "cols_single = []\n",
    "for col in tqdm(features):\n",
    "    unicnt = train_df[col].nunique()\n",
    "    if unicnt==1:\n",
    "        cols_single.append(col)\n",
    "print('->'*10, '唯一值特征: ', len(cols_single), cols_single)\n",
    "features = [f for f in features if f not in cols_single]\n",
    "print('->'*10, '去除唯一值后: ', len(features), features)\n",
    "\n",
    "# for col in features:\n",
    "#     num_5, num_95 = train_df[col].quantile(0.05), train_df[col].quantile(0.95)\n",
    "#     train_df[col] = train_df[col].apply(lambda x: num_5 if x<num_5 else num_95 if x>num_95 else x)\n",
    "\n",
    "            \n",
    "train = train_df.reset_index(drop=True).copy()\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x>-1 else -1)\n",
    "# train = train[train[col_label]!=0].reset_index(drop=True)\n",
    "test = test_df.reset_index(drop=True)\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "x_train = train[features]\n",
    "x_test = test[features]\n",
    "\n",
    "y_train = train[col_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['filename'].nunique(), test['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p',\n",
       "       'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew',\n",
       "       ...\n",
       "       'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p',\n",
       "       'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75',\n",
       "       'fea_a_6_skew', 'fea_a_6_kurt'],\n",
       "      dtype='object', length=116)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "0:\tlearn: 0.4461818\ttest: 0.4461863\tbest: 0.4461863 (0)\ttotal: 151ms\tremaining: 22m 42s\n",
      "500:\tlearn: 0.0128407\ttest: 0.0130008\tbest: 0.0130008 (500)\ttotal: 56.9s\tremaining: 16m 5s\n",
      "1000:\tlearn: 0.0119959\ttest: 0.0122678\tbest: 0.0122678 (1000)\ttotal: 1m 54s\tremaining: 15m 14s\n",
      "1500:\tlearn: 0.0115839\ttest: 0.0119553\tbest: 0.0119553 (1500)\ttotal: 2m 52s\tremaining: 14m 19s\n",
      "2000:\tlearn: 0.0113162\ttest: 0.0117834\tbest: 0.0117834 (2000)\ttotal: 3m 49s\tremaining: 13m 22s\n",
      "2500:\tlearn: 0.0111157\ttest: 0.0116731\tbest: 0.0116731 (2500)\ttotal: 4m 47s\tremaining: 12m 26s\n",
      "3000:\tlearn: 0.0109563\ttest: 0.0115939\tbest: 0.0115939 (3000)\ttotal: 5m 44s\tremaining: 11m 28s\n",
      "3500:\tlearn: 0.0108312\ttest: 0.0115362\tbest: 0.0115362 (3500)\ttotal: 6m 41s\tremaining: 10m 30s\n",
      "4000:\tlearn: 0.0107181\ttest: 0.0114921\tbest: 0.0114921 (4000)\ttotal: 7m 37s\tremaining: 9m 32s\n",
      "4500:\tlearn: 0.0106234\ttest: 0.0114561\tbest: 0.0114561 (4500)\ttotal: 8m 34s\tremaining: 8m 34s\n",
      "5000:\tlearn: 0.0105397\ttest: 0.0114280\tbest: 0.0114280 (4998)\ttotal: 9m 30s\tremaining: 7m 36s\n",
      "5500:\tlearn: 0.0104595\ttest: 0.0114043\tbest: 0.0114043 (5499)\ttotal: 10m 27s\tremaining: 6m 39s\n",
      "6000:\tlearn: 0.0103912\ttest: 0.0113861\tbest: 0.0113861 (6000)\ttotal: 11m 24s\tremaining: 5m 42s\n",
      "6500:\tlearn: 0.0103221\ttest: 0.0113690\tbest: 0.0113690 (6500)\ttotal: 12m 21s\tremaining: 4m 45s\n",
      "7000:\tlearn: 0.0102609\ttest: 0.0113529\tbest: 0.0113529 (7000)\ttotal: 13m 18s\tremaining: 3m 48s\n",
      "7500:\tlearn: 0.0102058\ttest: 0.0113436\tbest: 0.0113436 (7495)\ttotal: 14m 15s\tremaining: 2m 50s\n",
      "8000:\tlearn: 0.0101533\ttest: 0.0113328\tbest: 0.0113324 (7994)\ttotal: 15m 12s\tremaining: 1m 53s\n",
      "8500:\tlearn: 0.0101050\ttest: 0.0113237\tbest: 0.0113237 (8494)\ttotal: 16m 8s\tremaining: 56.9s\n",
      "8999:\tlearn: 0.0100580\ttest: 0.0113156\tbest: 0.0113156 (8999)\ttotal: 17m 5s\tremaining: 0us\n",
      "bestTest = 0.01131556534\n",
      "bestIteration = 8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:32<07:42, 92.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:06<06:13, 93.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:39<04:39, 93.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:11<03:05, 92.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:45<01:33, 93.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:51<00:00, 78.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7535952685976284]\n",
      "************************************ 2 ************************************\n",
      "0:\tlearn: 0.4461834\ttest: 0.4461751\tbest: 0.4461751 (0)\ttotal: 157ms\tremaining: 23m 34s\n",
      "500:\tlearn: 0.0128397\ttest: 0.0130297\tbest: 0.0130297 (500)\ttotal: 57.4s\tremaining: 16m 13s\n",
      "1000:\tlearn: 0.0119737\ttest: 0.0122825\tbest: 0.0122825 (1000)\ttotal: 1m 55s\tremaining: 15m 19s\n",
      "1500:\tlearn: 0.0115791\ttest: 0.0119879\tbest: 0.0119879 (1500)\ttotal: 2m 52s\tremaining: 14m 22s\n",
      "2000:\tlearn: 0.0113186\ttest: 0.0118182\tbest: 0.0118182 (2000)\ttotal: 3m 50s\tremaining: 13m 24s\n",
      "2500:\tlearn: 0.0111187\ttest: 0.0117022\tbest: 0.0117021 (2499)\ttotal: 4m 47s\tremaining: 12m 26s\n",
      "3000:\tlearn: 0.0109583\ttest: 0.0116223\tbest: 0.0116223 (3000)\ttotal: 5m 44s\tremaining: 11m 29s\n",
      "3500:\tlearn: 0.0108195\ttest: 0.0115633\tbest: 0.0115633 (3500)\ttotal: 6m 42s\tremaining: 10m 31s\n",
      "4000:\tlearn: 0.0107116\ttest: 0.0115191\tbest: 0.0115190 (3998)\ttotal: 7m 39s\tremaining: 9m 33s\n",
      "4500:\tlearn: 0.0106109\ttest: 0.0114781\tbest: 0.0114781 (4499)\ttotal: 8m 36s\tremaining: 8m 36s\n",
      "5000:\tlearn: 0.0105223\ttest: 0.0114474\tbest: 0.0114474 (5000)\ttotal: 9m 33s\tremaining: 7m 38s\n",
      "5500:\tlearn: 0.0104436\ttest: 0.0114250\tbest: 0.0114250 (5500)\ttotal: 10m 30s\tremaining: 6m 40s\n",
      "6000:\tlearn: 0.0103675\ttest: 0.0114052\tbest: 0.0114051 (5999)\ttotal: 11m 27s\tremaining: 5m 43s\n",
      "6500:\tlearn: 0.0102992\ttest: 0.0113863\tbest: 0.0113862 (6480)\ttotal: 12m 24s\tremaining: 4m 46s\n",
      "7000:\tlearn: 0.0102357\ttest: 0.0113716\tbest: 0.0113713 (6959)\ttotal: 13m 22s\tremaining: 3m 49s\n",
      "7500:\tlearn: 0.0101829\ttest: 0.0113588\tbest: 0.0113585 (7487)\ttotal: 14m 19s\tremaining: 2m 51s\n",
      "8000:\tlearn: 0.0101328\ttest: 0.0113492\tbest: 0.0113492 (8000)\ttotal: 15m 15s\tremaining: 1m 54s\n",
      "8500:\tlearn: 0.0100880\ttest: 0.0113399\tbest: 0.0113399 (8500)\ttotal: 16m 12s\tremaining: 57.1s\n",
      "8999:\tlearn: 0.0100389\ttest: 0.0113335\tbest: 0.0113334 (8993)\ttotal: 17m 9s\tremaining: 0us\n",
      "bestTest = 0.01133337552\n",
      "bestIteration = 8993\n",
      "Shrink model to first 8994 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:33<07:45, 93.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:08<06:16, 94.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:40<04:40, 93.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:12<03:06, 93.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:51<01:34, 94.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:57<00:00, 79.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7535952685976284, 0.7530654902376298]\n",
      "************************************ 3 ************************************\n",
      "0:\tlearn: 0.4461827\ttest: 0.4461827\tbest: 0.4461827 (0)\ttotal: 153ms\tremaining: 22m 59s\n",
      "500:\tlearn: 0.0128497\ttest: 0.0130503\tbest: 0.0130503 (500)\ttotal: 56.7s\tremaining: 16m 1s\n",
      "1000:\tlearn: 0.0119838\ttest: 0.0123020\tbest: 0.0123020 (1000)\ttotal: 1m 54s\tremaining: 15m 14s\n",
      "1500:\tlearn: 0.0115808\ttest: 0.0119950\tbest: 0.0119950 (1500)\ttotal: 2m 52s\tremaining: 14m 20s\n",
      "2000:\tlearn: 0.0113162\ttest: 0.0118257\tbest: 0.0118257 (2000)\ttotal: 3m 49s\tremaining: 13m 24s\n",
      "2500:\tlearn: 0.0111158\ttest: 0.0117157\tbest: 0.0117157 (2500)\ttotal: 4m 47s\tremaining: 12m 26s\n",
      "3000:\tlearn: 0.0109645\ttest: 0.0116433\tbest: 0.0116433 (3000)\ttotal: 5m 43s\tremaining: 11m 27s\n",
      "3500:\tlearn: 0.0108300\ttest: 0.0115769\tbest: 0.0115769 (3500)\ttotal: 6m 40s\tremaining: 10m 29s\n",
      "4000:\tlearn: 0.0107190\ttest: 0.0115325\tbest: 0.0115325 (4000)\ttotal: 7m 37s\tremaining: 9m 31s\n",
      "4500:\tlearn: 0.0106165\ttest: 0.0114996\tbest: 0.0114995 (4497)\ttotal: 8m 34s\tremaining: 8m 34s\n",
      "5000:\tlearn: 0.0105323\ttest: 0.0114711\tbest: 0.0114711 (5000)\ttotal: 9m 31s\tremaining: 7m 36s\n",
      "5500:\tlearn: 0.0104499\ttest: 0.0114471\tbest: 0.0114471 (5499)\ttotal: 10m 27s\tremaining: 6m 39s\n",
      "6000:\tlearn: 0.0103841\ttest: 0.0114277\tbest: 0.0114277 (6000)\ttotal: 11m 24s\tremaining: 5m 42s\n",
      "6500:\tlearn: 0.0103124\ttest: 0.0114097\tbest: 0.0114096 (6490)\ttotal: 12m 21s\tremaining: 4m 45s\n",
      "7000:\tlearn: 0.0102528\ttest: 0.0113958\tbest: 0.0113958 (6998)\ttotal: 13m 18s\tremaining: 3m 47s\n",
      "7500:\tlearn: 0.0101948\ttest: 0.0113866\tbest: 0.0113865 (7498)\ttotal: 14m 15s\tremaining: 2m 50s\n",
      "8000:\tlearn: 0.0101402\ttest: 0.0113737\tbest: 0.0113735 (7952)\ttotal: 15m 11s\tremaining: 1m 53s\n",
      "8500:\tlearn: 0.0100909\ttest: 0.0113655\tbest: 0.0113655 (8499)\ttotal: 16m 8s\tremaining: 56.8s\n",
      "bestTest = 0.01136078393\n",
      "bestIteration = 8859\n",
      "Shrink model to first 8860 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:32<07:41, 92.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:05<06:11, 92.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:37<04:37, 92.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:12<03:06, 93.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:53<01:36, 96.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:59<00:00, 79.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7535952685976284, 0.7530654902376298, 0.753084754905266]\n",
      "************************************ 4 ************************************\n",
      "0:\tlearn: 0.4461819\ttest: 0.4461908\tbest: 0.4461908 (0)\ttotal: 155ms\tremaining: 23m 10s\n",
      "500:\tlearn: 0.0128964\ttest: 0.0130226\tbest: 0.0130226 (500)\ttotal: 57.1s\tremaining: 16m 7s\n",
      "1000:\tlearn: 0.0120136\ttest: 0.0122615\tbest: 0.0122615 (1000)\ttotal: 1m 54s\tremaining: 15m 18s\n",
      "1500:\tlearn: 0.0115941\ttest: 0.0119447\tbest: 0.0119447 (1500)\ttotal: 2m 52s\tremaining: 14m 22s\n",
      "2000:\tlearn: 0.0113205\ttest: 0.0117700\tbest: 0.0117700 (2000)\ttotal: 3m 50s\tremaining: 13m 25s\n",
      "2500:\tlearn: 0.0111253\ttest: 0.0116596\tbest: 0.0116596 (2500)\ttotal: 4m 47s\tremaining: 12m 27s\n",
      "3000:\tlearn: 0.0109603\ttest: 0.0115716\tbest: 0.0115716 (2999)\ttotal: 5m 44s\tremaining: 11m 29s\n",
      "3500:\tlearn: 0.0108337\ttest: 0.0115159\tbest: 0.0115159 (3500)\ttotal: 6m 41s\tremaining: 10m 30s\n",
      "4000:\tlearn: 0.0107230\ttest: 0.0114689\tbest: 0.0114689 (4000)\ttotal: 7m 38s\tremaining: 9m 33s\n",
      "4500:\tlearn: 0.0106225\ttest: 0.0114308\tbest: 0.0114308 (4500)\ttotal: 8m 35s\tremaining: 8m 35s\n",
      "5000:\tlearn: 0.0105309\ttest: 0.0114012\tbest: 0.0114011 (4999)\ttotal: 9m 33s\tremaining: 7m 38s\n",
      "5500:\tlearn: 0.0104528\ttest: 0.0113784\tbest: 0.0113784 (5496)\ttotal: 10m 30s\tremaining: 6m 40s\n",
      "6000:\tlearn: 0.0103880\ttest: 0.0113597\tbest: 0.0113597 (6000)\ttotal: 11m 26s\tremaining: 5m 43s\n",
      "6500:\tlearn: 0.0103206\ttest: 0.0113413\tbest: 0.0113412 (6496)\ttotal: 12m 23s\tremaining: 4m 45s\n",
      "7000:\tlearn: 0.0102572\ttest: 0.0113257\tbest: 0.0113257 (7000)\ttotal: 13m 20s\tremaining: 3m 48s\n",
      "7500:\tlearn: 0.0101989\ttest: 0.0113153\tbest: 0.0113153 (7500)\ttotal: 14m 17s\tremaining: 2m 51s\n",
      "8000:\tlearn: 0.0101440\ttest: 0.0113037\tbest: 0.0113035 (7994)\ttotal: 15m 14s\tremaining: 1m 54s\n",
      "bestTest = 0.01130347436\n",
      "bestIteration = 7994\n",
      "Shrink model to first 7995 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:25<07:05, 85.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [02:53<05:48, 87.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:20<04:20, 86.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [05:48<02:54, 87.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:20<01:29, 89.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:26<00:00, 74.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7535952685976284, 0.7530654902376298, 0.753084754905266, 0.7539564811158097]\n",
      "************************************ 5 ************************************\n",
      "0:\tlearn: 0.4461842\ttest: 0.4461823\tbest: 0.4461823 (0)\ttotal: 157ms\tremaining: 23m 31s\n",
      "500:\tlearn: 0.0128593\ttest: 0.0130221\tbest: 0.0130221 (500)\ttotal: 57.2s\tremaining: 16m 10s\n",
      "1000:\tlearn: 0.0119929\ttest: 0.0122701\tbest: 0.0122701 (1000)\ttotal: 1m 54s\tremaining: 15m 16s\n",
      "1500:\tlearn: 0.0115779\ttest: 0.0119569\tbest: 0.0119569 (1500)\ttotal: 2m 52s\tremaining: 14m 21s\n",
      "2000:\tlearn: 0.0113116\ttest: 0.0117849\tbest: 0.0117849 (2000)\ttotal: 3m 50s\tremaining: 13m 25s\n",
      "2500:\tlearn: 0.0111048\ttest: 0.0116708\tbest: 0.0116708 (2500)\ttotal: 4m 48s\tremaining: 12m 29s\n",
      "3000:\tlearn: 0.0109488\ttest: 0.0115922\tbest: 0.0115922 (2997)\ttotal: 5m 45s\tremaining: 11m 30s\n",
      "3500:\tlearn: 0.0108173\ttest: 0.0115328\tbest: 0.0115328 (3500)\ttotal: 6m 42s\tremaining: 10m 32s\n",
      "4000:\tlearn: 0.0107036\ttest: 0.0114891\tbest: 0.0114891 (4000)\ttotal: 7m 40s\tremaining: 9m 34s\n",
      "4500:\tlearn: 0.0106091\ttest: 0.0114518\tbest: 0.0114518 (4500)\ttotal: 8m 36s\tremaining: 8m 36s\n",
      "5000:\tlearn: 0.0105252\ttest: 0.0114247\tbest: 0.0114246 (4991)\ttotal: 9m 33s\tremaining: 7m 38s\n",
      "5500:\tlearn: 0.0104454\ttest: 0.0114009\tbest: 0.0114009 (5500)\ttotal: 10m 30s\tremaining: 6m 41s\n",
      "6000:\tlearn: 0.0103690\ttest: 0.0113819\tbest: 0.0113819 (5997)\ttotal: 11m 28s\tremaining: 5m 43s\n",
      "6500:\tlearn: 0.0103065\ttest: 0.0113702\tbest: 0.0113702 (6491)\ttotal: 12m 24s\tremaining: 4m 46s\n",
      "7000:\tlearn: 0.0102512\ttest: 0.0113588\tbest: 0.0113588 (6998)\ttotal: 13m 21s\tremaining: 3m 48s\n",
      "7500:\tlearn: 0.0101939\ttest: 0.0113467\tbest: 0.0113467 (7494)\ttotal: 14m 18s\tremaining: 2m 51s\n",
      "8000:\tlearn: 0.0101426\ttest: 0.0113374\tbest: 0.0113374 (8000)\ttotal: 15m 15s\tremaining: 1m 54s\n",
      "8500:\tlearn: 0.0100924\ttest: 0.0113270\tbest: 0.0113269 (8497)\ttotal: 16m 12s\tremaining: 57.1s\n",
      "8999:\tlearn: 0.0100479\ttest: 0.0113220\tbest: 0.0113220 (8997)\ttotal: 17m 9s\tremaining: 0us\n",
      "bestTest = 0.01132204103\n",
      "bestIteration = 8997\n",
      "Shrink model to first 8998 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:34<07:51, 94.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:07<06:13, 93.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 3/6 [05:05<05:14, 104.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████▎                           | 4/6 [07:51<04:17, 128.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████▏             | 5/6 [09:25<01:56, 116.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:31<00:00, 95.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7535952685976284, 0.7530654902376298, 0.753084754905266, 0.7539564811158097, 0.7540227227849138]\n",
      "cab_score_list: [0.7535952685976284, 0.7530654902376298, 0.753084754905266, 0.7539564811158097, 0.7540227227849138]\n",
      "cab_score_mean: 0.7535449435282496\n",
      "cab_score_std: 0.00041032129929872936\n"
     ]
    }
   ],
   "source": [
    "sd = 1999110\n",
    "cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, cab_model_lst = cab_model(x_train, y_train, x_test, train_df['filename'].values, sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save model\n",
    "# for i in tqdm(range(len(cab_model_lst))):\n",
    "#     joblib.dump(cab_model_lst[i], '../model/sd1999110_feacut_10drop_v13_42_cab_{}.pkl'.format(i))\n",
    "\n",
    "# =============***************\n",
    "# infer way1- full infer\n",
    "# if you just want infer same result as me, please just load model and infer \n",
    "# =============***************\n",
    "# test = np.zeros((x_test.shape[0], len(np.unique(y_train))))    # 为多分类任务初始化\n",
    "# for i in tqdm(range(5)):\n",
    "#     model = joblib.load( '../model/sd1999110_feacut_10drop_v13_42_cab_{}.pkl'.format(i))\n",
    "#     print('load model {}'.format(i))\n",
    "#     test_pred_proba = model.predict_proba(x_test)\n",
    "#     test += test_pred_proba / 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del train_df, train,  test, x_train, y_train, x_test\n",
    "# [gc.collect() for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del combined_df, max_values_, mean_values_, combined_df_mge\n",
    "    [gc.collect() for _ in range(5)]\n",
    "except:\n",
    "    print('no need del')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no need del\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del combined_df, max_values_, mean_values_, combined_df_mge\n",
    "    [gc.collect() for _ in range(5)]\n",
    "except:\n",
    "    print('no need del')\n",
    "    \n",
    "# 合并概率矩阵和 file 列\n",
    "combined_df = pd.concat([test_df[['filename']].reset_index(drop=True), pd.DataFrame(1.0*cab_test )], axis=1)\n",
    "# combined_df = pd.concat([test_df[['filename']].reset_index(drop=True), pd.DataFrame( 0.6*cab_test_lst[1] + 0.4*cab_test_lst[0] )], axis=1)\n",
    "\n",
    "# 根据 'file' 列分组并计算 n 列的最大值\n",
    "max_values_ = combined_df.groupby('filename').max().reset_index()\n",
    "mean_values_ = combined_df.groupby('filename').mean().reset_index()\n",
    "\n",
    "combined_df_mge = pd.concat([max_values_, mean_values_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315720, 92)\n",
      "(315720, 92)\n"
     ]
    }
   ],
   "source": [
    "mge_values = combined_df_mge.groupby('filename').mean().reset_index()\n",
    "\n",
    "# mge_values.iloc[:, 3] = mge_values.iloc[:, 3]*0.85\n",
    "print(mge_values.shape)\n",
    "test_df_out = mge_values.copy()\n",
    "print(mge_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    72859\n",
      "2    38054\n",
      "1    36966\n",
      "9    14002\n",
      "5    13633\n",
      "Name: label_combined_num, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [01:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    303009\n",
      "1.0     11130\n",
      "0.5      1581\n",
      "Name: Zone_Air_Temperature_Sensor, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [00:18<00:00,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "col_label = 'label_combined_num'\n",
    "test_df_out[col_label] = np.argmax(mge_values.iloc[:, 1:].values, axis=1)\n",
    "test_df_out['prob'] = np.max(mge_values.iloc[:, 1:].values, axis=1)\n",
    "print(test_df_out[col_label].value_counts().head())\n",
    "\n",
    "file = 'v51_sd1999110_feacut_10drop_merge_v13_cab_5fold_5mean5max_42'\n",
    "test_df_out['label_combined'] = test_df_out['label_combined_num'].apply(lambda x: dict_num2str[x])\n",
    "for i in tqdm(range(len(cols_label))):\n",
    "    test_df_out[cols_label[i]] = test_df_out['label_combined'].apply(lambda x: int(x.split(',')[i])/2+0.5 )\n",
    "print(test_df_out['Zone_Air_Temperature_Sensor'].value_counts())\n",
    "\n",
    "for col in tqdm(cols_label):\n",
    "    test_df_out[col] = test_df_out[col].apply(lambda x: round(x,4))\n",
    "cols_out = ['filename'] + cols_label\n",
    "test_df_out[cols_out].to_csv('../res/{}.csv'.format(file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将 CSV 文件压缩为 ../tar/v51_sd1999110_feacut_10drop_merge_v13_cab_5fold_5mean5max_42.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 示例 CSV 文件列表\n",
    "csv_files = ['../res/{}.csv'.format(file)]  # 替换为你的 CSV 文件名\n",
    "output_filename = '../tar/{}.tar.gz'.format(file)\n",
    "\n",
    "# 调用函数\n",
    "compress_csv_to_tar_gz(csv_files, output_filename)\n",
    "\n",
    "print(f\"已将 CSV 文件压缩为 {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
