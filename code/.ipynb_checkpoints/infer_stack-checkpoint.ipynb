{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier as cab\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def compress_csv_to_tar_gz(csv_files, output_filename):\n",
    "    # 创建 tar.gz 文件\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        for csv_file in csv_files:\n",
    "            # 确保文件存在\n",
    "            if os.path.isfile(csv_file):\n",
    "                tar.add(csv_file, arcname=os.path.basename(csv_file))\n",
    "            else:\n",
    "                print(f\"文件 {csv_file} 不存在，跳过。\")\n",
    "                \n",
    "def reduce_mem_usage(df, only_fp64=False, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    if only_fp64==True:\n",
    "        numerics = [ 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def cv_model(clf, train_x, train_y, test_x, clf_name, train_y_2, sd):\n",
    "    folds = 5\n",
    "    seed = sd\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    # 初始化 GroupKFold，设置折数为 5\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "    nclass =  len(np.unique(train_y))\n",
    "    train = np.zeros((train_x.shape[0],nclass ))  # 为多分类任务初始化\n",
    "    test = np.zeros((test_x.shape[0], nclass ))    # 为多分类任务初始化\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "\n",
    "    model_lst = []\n",
    "    # 进行五折交叉验证\n",
    "#     for i, (train_index, valid_index) in enumerate(kf.split(train_x, groups=train_y_2)):\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i + 1)))\n",
    "        trn_x, trn_y = train_x.iloc[train_index], train_y[train_index]\n",
    "        val_x, val_y = train_x.iloc[valid_index], train_y[valid_index]\n",
    "#         sample_weight = y_train_sample_weight.iloc[train_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',  # multiclassova\n",
    "                'metric': 'multiclassova',  # 使用多分类的评价指标\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.1,\n",
    "                'seed': 2022,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "            model = clf.train(params, train_matrix, 5000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=500, early_stopping_rounds=500,\n",
    "                            # feval=WeightedF1Metric,  # 使用自定义 F1 评分函数\n",
    "                             )\n",
    "            val_pred_proba = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred_proba = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "        elif clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x, label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',  # 修改为多分类\n",
    "                'num_class': len(np.unique(train_y)),  # 类别数量\n",
    "                'eval_metric': 'mlogloss',  # 使用多分类的评价指标\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.3\n",
    "                ,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2020,\n",
    "                'n_jobs': -1,\n",
    "                \"silent\": True,\n",
    "                'tree_method': 'gpu_hist',      # 使用 GPU 加速\n",
    "                'predictor': 'gpu_predictor',\n",
    "            }\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = clf.train(params, train_matrix, 9000, evals=watchlist, \n",
    "                              verbose_eval=500, early_stopping_rounds=500)\n",
    "            val_pred_proba = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred_proba = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "#             print(val_pred_proba[:10])\n",
    "#             print(val_pred_proba[:10, :10])\n",
    "\n",
    "        elif clf_name == \"cab\":\n",
    "            params = {\n",
    "                'learning_rate': 0.3,\n",
    "                'l2_leaf_reg': 10,\n",
    "                'od_type': 'Iter',\n",
    "                'od_wait': 70,\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'random_seed': 11251,\n",
    "                'depth': 5,\n",
    "                'task_type': 'GPU',  # 启用 GPU\n",
    "                'loss_function': 'MultiClassOneVsAll',  # 修改为多分类\n",
    "                \n",
    "            }\n",
    "            model = clf(iterations=9000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      use_best_model=True, verbose=500,\n",
    "                      cat_features = [],\n",
    "#                       sample_weight=sample_weight  # 添加样本权重\n",
    "#                       custom_metric=[f1_metric]  # 使用自定义多分类 F1 作为评估指标\n",
    "                     )\n",
    "            \n",
    "            # 获取概率预测\n",
    "            val_pred_proba = model.predict_proba(val_x)\n",
    "            # test_pred_proba = model.predict_proba(test_x)\n",
    "            \n",
    "            \n",
    "            # 定义批大小\n",
    "            batch_size = 2000000  # 根据您的内存情况进行调整\n",
    "\n",
    "            # 计算总的样本数\n",
    "            num_samples = test_x.shape[0]\n",
    "\n",
    "            # 初始化一个空的数组来存储预测结果\n",
    "            test_pred_proba = np.empty((num_samples, nclass ))  # 假设 model.classes_ 返回类别数\n",
    "\n",
    "            # 分批次进行预测\n",
    "            for start in tqdm(range(0, num_samples, batch_size) ):\n",
    "                end = min(start + batch_size, num_samples)  # 确保不超出边界\n",
    "                print(start, end)\n",
    "\n",
    "                # 进行预测并直接存储到预测结果数组中\n",
    "                test_pred_proba[start:end] = model.predict_proba(test_x[start:end] )\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "        # 获取预测标签\n",
    "        val_pred = np.argmax(val_pred_proba, axis=1)\n",
    "#             test_labels = np.argmax(test_pred_proba, axis=1)\n",
    "\n",
    "        # 对于多分类，val_pred 和 test_pred 是类别的索引\n",
    "        train[valid_index] = val_pred_proba\n",
    "        test += test_pred_proba / kf.n_splits\n",
    "        \n",
    "\n",
    "\n",
    "        # 计算 F1 分数（可以根据需要选择其他多分类指标）\n",
    "        f1 = f1_score(val_y, val_pred, average='micro')  # 使用加权平均 F1 分数\n",
    "        cv_scores.append(f1)\n",
    "        \n",
    "\n",
    "\n",
    "#         print(val_y.iloc[:10])\n",
    "#         print(val_pred_proba[:10,:10])\n",
    "\n",
    "        print(cv_scores)\n",
    "        model_lst.append(model)\n",
    "        \n",
    "        del test_pred_proba, val_pred_proba, val_pred\n",
    "        [gc.collect() for _ in range(5)]\n",
    "        \n",
    "        if f1<0.7472:\n",
    "            break\n",
    "\n",
    "    print(\"%s_score_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train, test, cv_scores, np.mean(cv_scores), np.std(cv_scores), model_lst\n",
    "\n",
    "def lgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(lgb, x_train, y_train, x_test, \"lgb\", train_y_2, sd)\n",
    "    return lgb_train, lgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def cab_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(cab, x_train, y_train, x_test, \"cab\", train_y_2, sd)\n",
    "    return cab_train, cab_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst\n",
    "def xgb_model(x_train, y_train, x_test, train_y_2, sd):\n",
    "    xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst = cv_model(xgb, x_train, y_train, x_test, \"xgb\", train_y_2, sd)\n",
    "    return xgb_train, xgb_test, cv_scores, cv_scores_mean, cv_scores_std, model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1038169, 42), (10135596, 42))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "#     print('{}/{}'.format(root_dir, file))\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df = pd.concat([train_df, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df = pd.concat([test_df, df_], axis=0)\n",
    "\n",
    "train_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df.columns.tolist()[3:]]\n",
    "test_df.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df.columns.tolist()[3:]]\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038087, 41) (10121312, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_2'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_2'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038060, 51) (10116842, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_3'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_3'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038033, 39) (10112887, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../data_fea_sub/v13_4'\n",
    "file_lst_train = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'train' in filename][:]\n",
    "\n",
    "train_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_train):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    train_df_2 = pd.concat([train_df_2, df_], axis=0)\n",
    "\n",
    "root_dir = '../data_fea_sub/v13_4'\n",
    "file_lst_test = [filename for filename in os.listdir(root_dir) if filename.endswith('.pkl') and 'test' in filename][:]\n",
    "\n",
    "\n",
    "test_df_2 = pd.DataFrame()\n",
    "for file in tqdm(file_lst_test):\n",
    "    df_ = pd.read_pickle('{}/{}'.format(root_dir, file))\n",
    "    test_df_2 = pd.concat([test_df_2, df_], axis=0)\n",
    "\n",
    "train_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in train_df_2.columns.tolist()[3:]]\n",
    "test_df_2.columns = ['filename', 'dt_rk', 't_cut'] + ['fea_'+f for f in test_df_2.columns.tolist()[3:]]\n",
    "\n",
    "print(train_df_2.shape, test_df_2.shape)\n",
    "\n",
    "train_df_2 = train_df_2.iloc[:, :40]\n",
    "test_df_2 = test_df_2.iloc[:, :40]\n",
    "\n",
    "train_df = train_df.merge(train_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "test_df = test_df.merge(test_df_2, on = ['filename', 'dt_rk', 't_cut'], how='left')\n",
    "\n",
    "del train_df_2, test_df_2\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10135596, 164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['t_cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('../data/train_y_v0.1.0.csv')\n",
    "cols_label = train_label.columns.tolist()[1:]\n",
    "\n",
    "# 将指定列拼接成新的一列\n",
    "train_label['label_combined'] = train_label[cols_label].apply(lambda row: ', '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_combined_unique = train_label['label_combined'].value_counts().index.tolist()\n",
    "dict_str2num= {k: v for k, v in zip(label_combined_unique, range(len(label_combined_unique)))}\n",
    "dict_num2str= {k: v for k, v in zip(range(len(label_combined_unique)), label_combined_unique)}\n",
    "\n",
    "len(label_combined_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label['label_combined_num'] = train_label['label_combined'].apply(lambda x: dict_str2num[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_label, on='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1038169, 260), (10135596, 164))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['fea_count'] = train_df.groupby('filename')['filename'].transform('size')\n",
    "# test_df['fea_count'] = test_df.groupby('filename')['filename'].transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_drop = ['fea_t_max','fea_a_mean', 'fea_dt_p2p'\n",
    "             , 'fea_dt_unicnt','fea_dt_skew','fea_dt_std', 'fea_dt_min', 'fea_dt_max'\n",
    "            ,'fea_dt_count'\n",
    "            ,'fea_t_min','fea_v_count','fea_a_count'\n",
    "            ,'fea_t_p2p','fea_dt_Q50'\n",
    "             \n",
    "             \n",
    "             , 'fea_a_2_unicnt', 'fea_a_3_unicnt', 'fea_a_4_unicnt'\n",
    "             , 'fea_a_2_max', 'fea_a_3_max', 'fea_a_4_max'\n",
    "             , 'fea_a_Q50', 'fea_a_Q50', 'fea_a_Q50'\n",
    "#              ,'fea_dt_rk'             \n",
    "            ]\n",
    "\n",
    "train_df = train_df.drop(cols_drop, axis=1)\n",
    "test_df = test_df.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->->->->->->->->->-> 去除唯一值前:  116 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_a_2_sum', 'fea_a_2_mean', 'fea_a_2_std', 'fea_a_2_min', 'fea_a_2_p2p', 'fea_a_2_Q25', 'fea_a_2_Q50', 'fea_a_2_Q75', 'fea_a_2_skew', 'fea_a_2_kurt', 'fea_a_3_sum', 'fea_a_3_mean', 'fea_a_3_std', 'fea_a_3_min', 'fea_a_3_p2p', 'fea_a_3_Q25', 'fea_a_3_Q50', 'fea_a_3_Q75', 'fea_a_3_skew', 'fea_a_3_kurt', 'fea_a_4_sum', 'fea_a_4_mean', 'fea_a_4_std', 'fea_a_4_min', 'fea_a_4_p2p', 'fea_a_4_Q25', 'fea_a_4_Q50', 'fea_a_4_Q75', 'fea_a_4_skew', 'fea_a_4_kurt', 'fea_v_sum', 'fea_a_sum', 'fea_a_5_sum', 'fea_a_5_mean', 'fea_a_5_std', 'fea_a_5_max', 'fea_a_5_min', 'fea_a_5_p2p', 'fea_a_5_unicnt', 'fea_a_5_Q25', 'fea_a_5_Q50', 'fea_a_5_Q75', 'fea_a_5_skew', 'fea_a_5_kurt', 'fea_da_1_sum', 'fea_da_1_mean', 'fea_da_1_std', 'fea_da_1_max', 'fea_da_1_min', 'fea_da_1_p2p', 'fea_da_1_unicnt', 'fea_da_1_Q25', 'fea_da_1_Q50', 'fea_da_1_Q75', 'fea_da_1_skew', 'fea_da_1_kurt', 'fea_da_2_sum', 'fea_da_2_mean', 'fea_da_2_std', 'fea_da_2_max', 'fea_da_2_min', 'fea_da_2_p2p', 'fea_da_2_unicnt', 'fea_da_2_Q25', 'fea_da_2_Q50', 'fea_da_2_Q75', 'fea_da_2_skew', 'fea_da_2_kurt', 'fea_da_3_sum', 'fea_da_3_mean', 'fea_da_3_std', 'fea_da_3_max', 'fea_da_3_min', 'fea_da_3_p2p', 'fea_da_3_unicnt', 'fea_da_3_Q25', 'fea_da_3_Q50', 'fea_da_3_Q75', 'fea_da_3_skew', 'fea_da_3_kurt', 'fea_a_6_sum', 'fea_a_6_mean', 'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p', 'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75', 'fea_a_6_skew', 'fea_a_6_kurt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 116/116 [00:04<00:00, 28.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->->->->->->->->->-> 唯一值特征:  0 []\n",
      "->->->->->->->->->-> 去除唯一值后:  116 ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_a_2_sum', 'fea_a_2_mean', 'fea_a_2_std', 'fea_a_2_min', 'fea_a_2_p2p', 'fea_a_2_Q25', 'fea_a_2_Q50', 'fea_a_2_Q75', 'fea_a_2_skew', 'fea_a_2_kurt', 'fea_a_3_sum', 'fea_a_3_mean', 'fea_a_3_std', 'fea_a_3_min', 'fea_a_3_p2p', 'fea_a_3_Q25', 'fea_a_3_Q50', 'fea_a_3_Q75', 'fea_a_3_skew', 'fea_a_3_kurt', 'fea_a_4_sum', 'fea_a_4_mean', 'fea_a_4_std', 'fea_a_4_min', 'fea_a_4_p2p', 'fea_a_4_Q25', 'fea_a_4_Q50', 'fea_a_4_Q75', 'fea_a_4_skew', 'fea_a_4_kurt', 'fea_v_sum', 'fea_a_sum', 'fea_a_5_sum', 'fea_a_5_mean', 'fea_a_5_std', 'fea_a_5_max', 'fea_a_5_min', 'fea_a_5_p2p', 'fea_a_5_unicnt', 'fea_a_5_Q25', 'fea_a_5_Q50', 'fea_a_5_Q75', 'fea_a_5_skew', 'fea_a_5_kurt', 'fea_da_1_sum', 'fea_da_1_mean', 'fea_da_1_std', 'fea_da_1_max', 'fea_da_1_min', 'fea_da_1_p2p', 'fea_da_1_unicnt', 'fea_da_1_Q25', 'fea_da_1_Q50', 'fea_da_1_Q75', 'fea_da_1_skew', 'fea_da_1_kurt', 'fea_da_2_sum', 'fea_da_2_mean', 'fea_da_2_std', 'fea_da_2_max', 'fea_da_2_min', 'fea_da_2_p2p', 'fea_da_2_unicnt', 'fea_da_2_Q25', 'fea_da_2_Q50', 'fea_da_2_Q75', 'fea_da_2_skew', 'fea_da_2_kurt', 'fea_da_3_sum', 'fea_da_3_mean', 'fea_da_3_std', 'fea_da_3_max', 'fea_da_3_min', 'fea_da_3_p2p', 'fea_da_3_unicnt', 'fea_da_3_Q25', 'fea_da_3_Q50', 'fea_da_3_Q75', 'fea_da_3_skew', 'fea_da_3_kurt', 'fea_a_6_sum', 'fea_a_6_mean', 'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p', 'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75', 'fea_a_6_skew', 'fea_a_6_kurt']\n"
     ]
    }
   ],
   "source": [
    "col_label = 'label_combined_num'\n",
    "# 训练数据/测试数据准备\n",
    "features = ['fea_v_mean', 'fea_v_std', 'fea_v_max', 'fea_v_min', 'fea_v_p2p', 'fea_v_unicnt', 'fea_v_Q25', 'fea_v_Q50', 'fea_v_Q75', 'fea_v_skew', 'fea_v_kurt', 'fea_a_std', 'fea_a_max', 'fea_a_min', 'fea_a_p2p', 'fea_a_unicnt', 'fea_a_Q25', 'fea_a_Q75', 'fea_a_skew', 'fea_a_kurt', 'fea_dt_mean', 'fea_dt_Q25', 'fea_dt_Q75', 'fea_dt_kurt', 'fea_a_2_sum', 'fea_a_2_mean', 'fea_a_2_std', 'fea_a_2_min', 'fea_a_2_p2p', 'fea_a_2_Q25', 'fea_a_2_Q50', 'fea_a_2_Q75', 'fea_a_2_skew', 'fea_a_2_kurt', 'fea_a_3_sum', 'fea_a_3_mean', 'fea_a_3_std', 'fea_a_3_min', 'fea_a_3_p2p', 'fea_a_3_Q25', 'fea_a_3_Q50', 'fea_a_3_Q75', 'fea_a_3_skew', 'fea_a_3_kurt', 'fea_a_4_sum', 'fea_a_4_mean', 'fea_a_4_std', 'fea_a_4_min', 'fea_a_4_p2p', 'fea_a_4_Q25', 'fea_a_4_Q50', 'fea_a_4_Q75', 'fea_a_4_skew', 'fea_a_4_kurt', 'fea_v_sum', 'fea_a_sum', 'fea_a_5_sum', 'fea_a_5_mean', 'fea_a_5_std', 'fea_a_5_max', 'fea_a_5_min', 'fea_a_5_p2p', 'fea_a_5_unicnt', 'fea_a_5_Q25', 'fea_a_5_Q50', 'fea_a_5_Q75', 'fea_a_5_skew', 'fea_a_5_kurt', 'fea_da_1_sum', 'fea_da_1_mean', 'fea_da_1_std', 'fea_da_1_max', 'fea_da_1_min', 'fea_da_1_p2p', 'fea_da_1_unicnt', 'fea_da_1_Q25', 'fea_da_1_Q50', 'fea_da_1_Q75', 'fea_da_1_skew', 'fea_da_1_kurt', 'fea_da_2_sum', 'fea_da_2_mean', 'fea_da_2_std', 'fea_da_2_max', 'fea_da_2_min', 'fea_da_2_p2p', 'fea_da_2_unicnt', 'fea_da_2_Q25', 'fea_da_2_Q50', 'fea_da_2_Q75', 'fea_da_2_skew', 'fea_da_2_kurt', 'fea_da_3_sum', 'fea_da_3_mean', 'fea_da_3_std'\n",
    "            , 'fea_da_3_max', 'fea_da_3_min', 'fea_da_3_p2p', 'fea_da_3_unicnt', 'fea_da_3_Q25', 'fea_da_3_Q50', 'fea_da_3_Q75', 'fea_da_3_skew', 'fea_da_3_kurt', 'fea_a_6_sum', 'fea_a_6_mean', 'fea_a_6_std', 'fea_a_6_max', 'fea_a_6_min', 'fea_a_6_p2p', 'fea_a_6_unicnt', 'fea_a_6_Q25', 'fea_a_6_Q50', 'fea_a_6_Q75', 'fea_a_6_skew', 'fea_a_6_kurt']\n",
    "print('->'*10, '去除唯一值前: ', len(features), features)\n",
    "cols_single = []\n",
    "for col in tqdm(features):\n",
    "    unicnt = train_df[col].nunique()\n",
    "    if unicnt==1:\n",
    "        cols_single.append(col)\n",
    "print('->'*10, '唯一值特征: ', len(cols_single), cols_single)\n",
    "features = [f for f in features if f not in cols_single]\n",
    "print('->'*10, '去除唯一值后: ', len(features), features)\n",
    "\n",
    "# for col in features:\n",
    "#     num_5, num_95 = train_df[col].quantile(0.05), train_df[col].quantile(0.95)\n",
    "#     train_df[col] = train_df[col].apply(lambda x: num_5 if x<num_5 else num_95 if x>num_95 else x)\n",
    "\n",
    "            \n",
    "# train = train_df.reset_index(drop=True).copy()\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x>-1 else -1)\n",
    "# train = train[train[col_label]!=0].reset_index(drop=True)\n",
    "# test = test_df.reset_index(drop=True)\n",
    "# train[col_label] = train[col_label].apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "# x_train = train[features]\n",
    "x_test = test_df[features].reset_index(drop=True)\n",
    "\n",
    "y_train = train_df[col_label].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_info = test_df[['filename']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df, test_df\n",
    "[gc.collect() for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:31<07:39, 91.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:04<06:09, 92.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:37<04:37, 92.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:10<03:05, 92.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:43<01:32, 92.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:49<00:00, 78.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:33<07:45, 93.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:05<06:10, 92.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:38<04:38, 92.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:13<03:07, 93.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:47<01:33, 93.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:53<00:00, 78.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:31<07:39, 91.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:03<06:05, 91.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:36<04:37, 92.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:08<03:04, 92.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:40<01:32, 92.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:47<00:00, 77.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:31<07:36, 91.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [02:57<05:53, 88.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:24<04:22, 87.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [05:49<02:53, 86.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:15<01:26, 86.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:20<00:00, 73.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 1/6 [01:32<07:44, 92.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 2/6 [03:05<06:10, 92.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000000 6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 3/6 [04:38<04:38, 92.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 8000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [06:10<03:05, 92.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [07:42<01:32, 92.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 10135596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:49<00:00, 78.19s/it]\n"
     ]
    }
   ],
   "source": [
    "test_pred_proba_lst = []\n",
    "nclass =  len(np.unique(y_train))\n",
    "for i in range(5):\n",
    "    model = joblib.load( '../model/sd1999110_feacut_10drop_v13_42_cab_{}.pkl'.format(i))\n",
    "    print('load model {}'.format(i))\n",
    "    # 定义批大小\n",
    "    batch_size = 2000000  # 根据您的内存情况进行调整\n",
    "\n",
    "    # 计算总的样本数\n",
    "    num_samples = x_test.shape[0]\n",
    "\n",
    "    # 初始化一个空的数组来存储预测结果\n",
    "    test_pred_proba = np.empty((num_samples, nclass ))  # 假设 model.classes_ 返回类别数\n",
    "\n",
    "    # 分批次进行预测\n",
    "    for start in tqdm(range(0, num_samples, batch_size) ):\n",
    "        end = min(start + batch_size, num_samples)  # 确保不超出边界\n",
    "        print(start, end)\n",
    "\n",
    "        # 进行预测并直接存储到预测结果数组中\n",
    "        test_pred_proba[start:end] = model.predict_proba(x_test[start:end] )\n",
    "    test_pred_proba_lst.append(test_pred_proba)\n",
    "    \n",
    "    del test_pred_proba\n",
    "    [gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_test\n",
    "[gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = test_pred_proba_lst[0].shape[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10135596"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:37<00:00, 43.44s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del test\n",
    "    [gc.collect() for _ in range(5)]\n",
    "except:\n",
    "    print('no need del')\n",
    "    \n",
    "test = np.zeros((m, nclass))    # 为多分类任务初始化\n",
    "for i in tqdm([0,1,2,3,4]):\n",
    "    if i in [1]:\n",
    "        test += test_pred_proba_lst[i] *0.4\n",
    "    else:\n",
    "        test += test_pred_proba_lst[i] *0.2\n",
    "        \n",
    "    [gc.collect() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del combined_df, max_values_, mean_values_, combined_df_mge\n",
    "    [gc.collect() for _ in range(5)]\n",
    "except:\n",
    "    print('no need del')\n",
    "    \n",
    "# 合并概率矩阵和 file 列\n",
    "combined_df = pd.concat([test_df_info, pd.DataFrame(1.0*test )], axis=1)\n",
    "# combined_df = pd.concat([test_df[['filename']].reset_index(drop=True), pd.DataFrame( 0.6*cab_test_lst[1] + 0.4*cab_test_lst[0] )], axis=1)\n",
    "\n",
    "# 根据 'file' 列分组并计算 n 列的最大值\n",
    "max_values_ = combined_df.groupby('filename').max().reset_index()\n",
    "mean_values_ = combined_df.groupby('filename').mean().reset_index()\n",
    "\n",
    "combined_df_mge = pd.concat([max_values_, mean_values_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315720, 92)\n",
      "(315720, 92)\n"
     ]
    }
   ],
   "source": [
    "mge_values = combined_df_mge.groupby('filename').mean().reset_index()\n",
    "\n",
    "# mge_values.iloc[:, 3] = mge_values.iloc[:, 3]*0.85\n",
    "print(mge_values.shape)\n",
    "test_df_out = mge_values.copy()\n",
    "print(mge_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    72653\n",
      "2    38222\n",
      "1    36581\n",
      "9    14371\n",
      "5    13641\n",
      "Name: label_combined_num, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [01:37<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    303052\n",
      "1.0     11082\n",
      "0.5      1586\n",
      "Name: Zone_Air_Temperature_Sensor, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 94/94 [00:18<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "col_label = 'label_combined_num'\n",
    "test_df_out[col_label] = np.argmax(mge_values.iloc[:, 1:].values, axis=1)\n",
    "test_df_out['prob'] = np.max(mge_values.iloc[:, 1:].values, axis=1)\n",
    "print(test_df_out[col_label].value_counts().head())\n",
    "\n",
    "file = 'v51_sd1999110_feacut_10drop_merge_v13_cab_5fold_5mean5max_42_stack1bigger'\n",
    "test_df_out['label_combined'] = test_df_out['label_combined_num'].apply(lambda x: dict_num2str[x])\n",
    "for i in tqdm(range(len(cols_label))):\n",
    "    test_df_out[cols_label[i]] = test_df_out['label_combined'].apply(lambda x: int(x.split(',')[i])/2+0.5 )\n",
    "print(test_df_out['Zone_Air_Temperature_Sensor'].value_counts())\n",
    "\n",
    "for col in tqdm(cols_label):\n",
    "    test_df_out[col] = test_df_out[col].apply(lambda x: round(x,4))\n",
    "cols_out = ['filename'] + cols_label\n",
    "test_df_out[cols_out].to_csv('../res/{}.csv'.format(file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将 CSV 文件压缩为 ../tar/v51_sd7_feacut_10drop_merge_v13_cab_5fold_5mean5max_42_stack1bigger.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# 示例 CSV 文件列表\n",
    "csv_files = ['../res/{}.csv'.format(file)]  # 替换为你的 CSV 文件名\n",
    "output_filename = '../tar/{}.tar.gz'.format(file)\n",
    "\n",
    "# 调用函数\n",
    "compress_csv_to_tar_gz(csv_files, output_filename)\n",
    "\n",
    "print(f\"已将 CSV 文件压缩为 {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.616\t0.574\t0.692\t0.615\t0.989\n",
    "# v51_sd7_feacut_10drop_merge_v13_cab_5fold_5mean5max_42_stack1bigger\n",
    "# for i in tqdm([0,1,2,3,4]):\n",
    "#     if i in [1]:\n",
    "#         test += test_pred_proba_lst[i] *0.4\n",
    "#     else:\n",
    "#         test += test_pred_proba_lst[i] *0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
